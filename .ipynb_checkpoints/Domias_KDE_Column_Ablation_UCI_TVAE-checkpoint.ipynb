{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sdv.single_table import TVAESynthesizer\n",
    "\n",
    "from domias.evaluator import evaluate_performance\n",
    "from domias.models.generator import GeneratorInterface\n",
    "from domias.models.ctgan import CTGAN\n",
    "from sdv.metadata import SingleTableMetadata\n",
    "\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from itertools import zip_longest\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import fetch_california_housing, fetch_covtype, load_digits\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LIMIT_BAL</th>\n",
       "      <th>SEX</th>\n",
       "      <th>EDUCATION</th>\n",
       "      <th>MARRIAGE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PAY_0</th>\n",
       "      <th>PAY_2</th>\n",
       "      <th>PAY_3</th>\n",
       "      <th>PAY_4</th>\n",
       "      <th>PAY_5</th>\n",
       "      <th>...</th>\n",
       "      <th>BILL_AMT4</th>\n",
       "      <th>BILL_AMT5</th>\n",
       "      <th>BILL_AMT6</th>\n",
       "      <th>PAY_AMT1</th>\n",
       "      <th>PAY_AMT2</th>\n",
       "      <th>PAY_AMT3</th>\n",
       "      <th>PAY_AMT4</th>\n",
       "      <th>PAY_AMT5</th>\n",
       "      <th>PAY_AMT6</th>\n",
       "      <th>default.payment.next.month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>689.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>120000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3272.0</td>\n",
       "      <td>3455.0</td>\n",
       "      <td>3261.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>14331.0</td>\n",
       "      <td>14948.0</td>\n",
       "      <td>15549.0</td>\n",
       "      <td>1518.0</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>28314.0</td>\n",
       "      <td>28959.0</td>\n",
       "      <td>29547.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>1069.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>20940.0</td>\n",
       "      <td>19146.0</td>\n",
       "      <td>19131.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>36681.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>9000.0</td>\n",
       "      <td>689.0</td>\n",
       "      <td>679.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   LIMIT_BAL  SEX  EDUCATION  MARRIAGE  AGE  PAY_0  PAY_2  PAY_3  PAY_4  \\\n",
       "0    20000.0    2          2         1   24      2      2     -1     -1   \n",
       "1   120000.0    2          2         2   26     -1      2      0      0   \n",
       "2    90000.0    2          2         2   34      0      0      0      0   \n",
       "3    50000.0    2          2         1   37      0      0      0      0   \n",
       "4    50000.0    1          2         1   57     -1      0     -1      0   \n",
       "\n",
       "   PAY_5  ...  BILL_AMT4  BILL_AMT5  BILL_AMT6  PAY_AMT1  PAY_AMT2  PAY_AMT3  \\\n",
       "0     -2  ...        0.0        0.0        0.0       0.0     689.0       0.0   \n",
       "1      0  ...     3272.0     3455.0     3261.0       0.0    1000.0    1000.0   \n",
       "2      0  ...    14331.0    14948.0    15549.0    1518.0    1500.0    1000.0   \n",
       "3      0  ...    28314.0    28959.0    29547.0    2000.0    2019.0    1200.0   \n",
       "4      0  ...    20940.0    19146.0    19131.0    2000.0   36681.0   10000.0   \n",
       "\n",
       "   PAY_AMT4  PAY_AMT5  PAY_AMT6  default.payment.next.month  \n",
       "0       0.0       0.0       0.0                           1  \n",
       "1    1000.0       0.0    2000.0                           1  \n",
       "2    1000.0    1000.0    5000.0                           0  \n",
       "3    1100.0    1069.0    1000.0                           0  \n",
       "4    9000.0     689.0     679.0                           0  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import dataset\n",
    "df_uci = pd.read_csv(r'C:\\Users\\jordy\\OneDrive\\MSc_Python\\Individual_Project\\Data\\UCI_Credit_Card.csv')\n",
    "df_uci.drop(columns=['ID'], inplace=True)\n",
    "df_uci.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 24)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #convert dataframe to array\n",
    "# arr_uci = np.array(df_uci.iloc[:, 1:])\n",
    "# arr_uci.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_dataset() -> np.ndarray:\n",
    "#     def data_loader() -> np.ndarray:\n",
    "#         scaler = StandardScaler()\n",
    "#         X =arr_uci\n",
    "#         np.random.shuffle(X)\n",
    "#         return scaler.fit_transform(X)\n",
    "\n",
    "#     return data_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_generator(\n",
    "    gan_method: str = \"CTGAN\",\n",
    "    epochs: int = 1000,\n",
    "    seed: int = 0,\n",
    ") -> GeneratorInterface:\n",
    "    class LocalGenerator(GeneratorInterface):\n",
    "        def __init__(self) -> None:\n",
    "            if gan_method == \"TVAE\":\n",
    "                syn_model = TVAESynthesizer(metadata, epochs=epochs)\n",
    "            elif gan_method == \"CTGAN\":\n",
    "                syn_model = CTGAN(epochs=epochs)\n",
    "            elif gan_method == \"KDE\":\n",
    "                syn_model = None\n",
    "            else:\n",
    "                raise RuntimeError()\n",
    "            self.method = gan_method\n",
    "            self.model = syn_model\n",
    "                \n",
    "\n",
    "        def fit(self, data: pd.DataFrame) -> \"LocalGenerator\":\n",
    "            if self.method == \"KDE\":\n",
    "                self.model = stats.gaussian_kde(np.transpose(data))\n",
    "            else:\n",
    "                self.model.fit(data)\n",
    "            return self\n",
    "\n",
    "        def generate(self, count: int) -> pd.DataFrame:\n",
    "            \n",
    "            if gan_method == \"KDE\":\n",
    "                samples = pd.DataFrame(self.model.resample(count).transpose(1, 0))\n",
    "            elif gan_method == \"TVAE\":\n",
    "                samples = self.model.sample(count)\n",
    "            elif gan_method == \"CTGAN\":\n",
    "                samples = self.model.generate(count)\n",
    "            else:\n",
    "                raise RuntimeError()\n",
    "\n",
    "            return samples\n",
    "            \n",
    "            #return self.model.sample(count)\n",
    "\n",
    "    return LocalGenerator()\n",
    "\n",
    "\n",
    "#Loading metadata from dataset for use in TVAESynthesizer\n",
    "\n",
    "# from sdv.metadata import SingleTableMetadata\n",
    "\n",
    "# metadata = SingleTableMetadata()\n",
    "# metadata.detect_from_dataframe(data=df_dataset)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_generator(\n",
    "#     gan_method: str = \"CTGAN\",\n",
    "#     metadata: SingleTableMetadata = None,\n",
    "#     epochs: int = 1000,\n",
    "#     seed: int = 0,\n",
    "# ) -> GeneratorInterface:\n",
    "#     class LocalGenerator(GeneratorInterface):\n",
    "#         def __init__(self, metadata=None) -> None:\n",
    "#             if gan_method == \"TVAE\":\n",
    "#                 self.model = TVAESynthesizer(metadata=metadata, epochs=epochs)\n",
    "#             elif gan_method == \"CTGAN\":\n",
    "#                 self.model = CTGAN(epochs=epochs)\n",
    "#             elif gan_method == \"KDE\":\n",
    "#                 self.model = None\n",
    "#             else:\n",
    "#                 raise RuntimeError(\"Unknown GAN method specified.\")\n",
    "#             self.method = gan_method\n",
    "#             self.metadata = metadata\n",
    "\n",
    "#         def fit(self, data: pd.DataFrame) -> \"LocalGenerator\":\n",
    "#             if self.method == \"KDE\":\n",
    "#                 self.model = stats.gaussian_kde(np.transpose(data))\n",
    "#             else:\n",
    "#                 self.model.fit(data)\n",
    "#             return self\n",
    "\n",
    "#         def generate(self, count: int) -> pd.DataFrame:\n",
    "#             if self.method == \"KDE\":\n",
    "#                 samples = pd.DataFrame(self.model.resample(count).transpose(1, 0))\n",
    "#             elif self.method == \"TVAE\":\n",
    "#                 samples = self.model.sample(count)\n",
    "#             elif self.method == \"CTGAN\":\n",
    "#                 samples = self.model.generate(count)\n",
    "#             else:\n",
    "#                 raise RuntimeError(\"Unknown GAN method specified.\")\n",
    "#             return samples\n",
    "\n",
    "#     return LocalGenerator(metadata=metadata)\n",
    "\n",
    "\n",
    "\n",
    "# #Loading metadata from dataset for use in TVAESynthesizer\n",
    "# dataset = get_dataset()\n",
    "# df_dataset = pd.DataFrame(dataset)\n",
    "# df_dataset.rename(columns={0: '0', 1: '1', 2: '2', 3: '3', 4: '4', 5: '5', 6: '6', 7: '7', 8:'8', 9:'9', 10:'10', 11:'11', 12:'12',\n",
    "#                           13:'13', 14:'14', 15:'15', 16:'16', 17:'17', 18:'18', 19:'19', 20:'20', 21:'21', 22:'22', 23:'23'}, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = get_dataset()\n",
    "# df_dataset = pd.DataFrame(dataset)\n",
    "# df_dataset.rename(columns={0: '0', 1: '1', 2: '2', 3: '3', 4: '4', 5: '5', 6: '6', 7: '7', 8:'8', 9:'9', 10:'10', 11:'11', 12:'12',\n",
    "#                           13:'13', 14:'14', 15:'15', 16:'16', 17:'17', 18:'18', 19:'19', 20:'20', 21:'21', 22:'22', 23:'23', 24:'24'}, inplace = True)\n",
    "\n",
    "\n",
    "# #Create loop to create a metadta object for each column ablated dataframe\n",
    "# metadata_objects={}\n",
    "# for i, col in enumerate(df_dataset.columns):\n",
    "    \n",
    "#     df_temp = df_dataset.copy()\n",
    "#     df_temp.drop(columns=[col], inplace=True)\n",
    "    \n",
    "#     metadata_name = f\"metadata_{col}\"\n",
    "#     metadata_objects[metadata_name] = SingleTableMetadata()\n",
    "#     metadata_objects[metadata_name].detect_from_dataframe(data=df_temp)\n",
    "\n",
    "# #metadata_objects[metadata_1]\n",
    "# #print(type(metadata_objects['metadata_1']))\n",
    "# #metadata_objects['metadata_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "metadata_0\n",
      "{\n",
      "    \"columns\": {\n",
      "        \"1\": {\n",
      "            \"sdtype\": \"numerical\"\n",
      "        },\n",
      "        \"2\": {\n",
      "            \"sdtype\": \"numerical\"\n",
      "        },\n",
      "        \"3\": {\n",
      "            \"sdtype\": \"numerical\"\n",
      "        },\n",
      "        \"4\": {\n",
      "            \"sdtype\": \"numerical\"\n",
      "        },\n",
      "        \"5\": {\n",
      "            \"sdtype\": \"numerical\"\n",
      "        },\n",
      "        \"6\": {\n",
      "            \"sdtype\": \"numerical\"\n",
      "        },\n",
      "        \"7\": {\n",
      "            \"sdtype\": \"numerical\"\n",
      "        },\n",
      "        \"8\": {\n",
      "            \"sdtype\": \"numerical\"\n",
      "        },\n",
      "        \"9\": {\n",
      "            \"sdtype\": \"numerical\"\n",
      "        },\n",
      "        \"10\": {\n",
      "            \"sdtype\": \"numerical\"\n",
      "        },\n",
      "        \"11\": {\n",
      "            \"sdtype\": \"numerical\"\n",
      "        },\n",
      "        \"12\": {\n",
      "            \"sdtype\": \"numerical\"\n",
      "        },\n",
      "        \"13\": {\n",
      "            \"sdtype\": \"numerical\"\n",
      "        },\n",
      "        \"14\": {\n",
      "            \"sdtype\": \"numerical\"\n",
      "        },\n",
      "        \"15\": {\n",
      "            \"sdtype\": \"numerical\"\n",
      "        },\n",
      "        \"16\": {\n",
      "            \"sdtype\": \"numerical\"\n",
      "        },\n",
      "        \"17\": {\n",
      "            \"sdtype\": \"numerical\"\n",
      "        },\n",
      "        \"18\": {\n",
      "            \"sdtype\": \"numerical\"\n",
      "        },\n",
      "        \"19\": {\n",
      "            \"sdtype\": \"numerical\"\n",
      "        },\n",
      "        \"20\": {\n",
      "            \"sdtype\": \"numerical\"\n",
      "        },\n",
      "        \"21\": {\n",
      "            \"sdtype\": \"numerical\"\n",
      "        },\n",
      "        \"22\": {\n",
      "            \"sdtype\": \"numerical\"\n",
      "        },\n",
      "        \"23\": {\n",
      "            \"sdtype\": \"numerical\"\n",
      "        }\n",
      "    },\n",
      "    \"METADATA_SPEC_VERSION\": \"SINGLE_TABLE_V1\"\n",
      "}\n",
      "Index(['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13',\n",
      "       '14', '15', '16', '17', '18', '19', '20', '21', '22', '23'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "ename": "InvalidDataError",
     "evalue": "The provided data does not match the metadata:\nThe columns ['0'] are not present in the metadata.\n\nThe metadata columns ['23'] are not present in the data.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidDataError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-5bc4289f89e1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     64\u001b[0m                 )\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m                 perf = evaluate_performance(\n\u001b[0m\u001b[0;32m     67\u001b[0m                     \u001b[0mgenerator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m                     \u001b[0marr_temp\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\domias\\evaluator.py\u001b[0m in \u001b[0;36mevaluate_performance\u001b[1;34m(generator, dataset, mem_set_size, reference_set_size, training_epochs, synthetic_sizes, density_estimator, seed, device, shifted_column, zero_quantile, reference_kept_p)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m     \u001b[1;31m# Train generator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 190\u001b[1;33m     \u001b[0mgenerator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0msynthetic_size\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msynthetic_sizes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-30-dad392754b94>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     22\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstats\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgaussian_kde\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sdv\\single_table\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    484\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data_processor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_sampling\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    485\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_random_state_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 486\u001b[1;33m         \u001b[0mprocessed_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_preprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    487\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_processed_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessed_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sdv\\single_table\\base.py\u001b[0m in \u001b[0;36m_preprocess\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_preprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 432\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    433\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data_processor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data_processor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sdv\\single_table\\base.py\u001b[0m in \u001b[0;36mvalidate\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m         \u001b[1;31m# Both metadata and data must have the same set of columns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 240\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_metadata_matches_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    241\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m         \u001b[0merrors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sdv\\single_table\\base.py\u001b[0m in \u001b[0;36m_validate_metadata_matches_data\u001b[1;34m(self, columns)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mInvalidDataError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_primary_and_alternate_keys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidDataError\u001b[0m: The provided data does not match the metadata:\nThe columns ['0'] are not present in the metadata.\n\nThe metadata columns ['23'] are not present in the data."
     ]
    }
   ],
   "source": [
    "\n",
    "# # #Create loop to create a metadta object for each column ablated dataframe\n",
    "# # metadata_objects={}\n",
    "# # for i, col in enumerate(df_dataset.columns):\n",
    "    \n",
    "# #     df_temp = df_dataset.copy()\n",
    "# #     df_temp.drop(columns=[col], inplace=True)\n",
    "    \n",
    "# #     metadata_name = f\"metadata_{col}\"\n",
    "# #     metadata_objects[metadata_name] = SingleTableMetadata()\n",
    "# #     metadata_objects[metadata_name].detect_from_dataframe(data=df_temp)\n",
    "\n",
    "# # mem_set_size = 1000 -> originally what training size was\n",
    "# reference_set_size = 10000 #held out set\n",
    "# training_epochs = [2000]\n",
    "# training_sizes = [10000]\n",
    "# #synthetic_sizes = [200]\n",
    "# density_estimator = \"kde\"  # prior, kde, bnaf\n",
    "# gen_size = 500 #same as synthetic_sizes\n",
    "\n",
    "# method = \"TVAE\"\n",
    "\n",
    "# leave_one_out_results = {}\n",
    "\n",
    "\n",
    "# for col, key in zip(df_dataset.columns, metadata_objects.keys()):\n",
    "#     print(col)\n",
    "#     print(key)\n",
    "    \n",
    "    \n",
    "#     metadata = metadata_objects[key]\n",
    "#     #print(type(metadata))\n",
    "#     print(metadata)\n",
    "    \n",
    "#     df_temp = df_dataset.drop(columns=[col])\n",
    "# #     df_temp.drop(columns=[col], inplace=True)\n",
    "# #     df_temp.rename(columns={'0': 0}, inplace=True)\n",
    "#     print(df_temp.columns)\n",
    "#     arr_temp = np.array(df_temp)\n",
    "    \n",
    "#     #print(arr_temp)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "#     # Create a dictionary to store the results for the current column\n",
    "#     column_results = {}\n",
    "\n",
    "#     # Set the number of iterations\n",
    "#     num_iterations = 10\n",
    "\n",
    "#     for iteration in range(1, num_iterations+1):\n",
    "#         # Initialize the result dictionary for the current iteration\n",
    "#         iteration_results = {}\n",
    "\n",
    "#         for training_size in training_sizes:\n",
    "#             # Initialize the result dictionary for the current training size\n",
    "#             size_results = {}\n",
    "\n",
    "#             for training_epoch in training_epochs:\n",
    "#                 generator = get_generator(\n",
    "#                     gan_method=method,\n",
    "#                     epochs=training_epoch,\n",
    "#                 )\n",
    "\n",
    "#                 perf = evaluate_performance(\n",
    "#                     generator,\n",
    "#                     arr_temp,\n",
    "#                     training_size,\n",
    "#                     reference_set_size,\n",
    "#                     training_epochs=training_epoch,\n",
    "#                     synthetic_sizes=[gen_size],\n",
    "#                     density_estimator=density_estimator,\n",
    "#                 )\n",
    "\n",
    "#                 # Store the MIA performance for the current training size and epoch\n",
    "#                 size_results[training_epoch] = perf[gen_size][\"MIA_performance\"]\n",
    "\n",
    "#             # Store the results for the current training size\n",
    "#             iteration_results[training_size] = size_results\n",
    "\n",
    "#         # Store the results for the current iteration\n",
    "#         column_results[iteration] = iteration_results\n",
    "    \n",
    "#     # Store the results for the current column\n",
    "#     leave_one_out_results[col] = column_results\n",
    "    \n",
    "\n",
    "# # Print the results\n",
    "# for col, results in leave_one_out_results.items():\n",
    "#     print(f\"Column Omitted {col}:\")\n",
    "#     for iteration, iteration_results in results.items():\n",
    "#         print(f\"Iteration {iteration}:\")\n",
    "#         for training_size, size_results in iteration_results.items():\n",
    "#             print(f\"Training Size {training_size}:\")\n",
    "#             for training_epoch, mia_performance in size_results.items():\n",
    "#                 print(f\"Training Epoch {training_epoch}: MIA Performance = {mia_performance}\")\n",
    "#             print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create empty lists to store the results\n",
    "# iterations = []\n",
    "# epochs = []\n",
    "# srcs = []\n",
    "# aucrocs = []\n",
    "# column_ablated = []\n",
    "\n",
    "# # Iterate over the results\n",
    "# for col, results in leave_one_out_results.items():\n",
    "#     for iteration, iteration_results in results.items():\n",
    "#         for training_size, size_results in iteration_results.items():\n",
    "#             for training_epoch, mia_performance in size_results.items():\n",
    "#                 # Append values to the lists\n",
    "#                 iterations.append(iteration)\n",
    "#                 epochs.append(training_epoch)\n",
    "#                 srcs.append(list(mia_performance.keys())[8])\n",
    "#                 aucrocs.append(list(mia_performance.values())[8]['aucroc'])\n",
    "#                 column_ablated.append(col)  # Add the column_ablated value\n",
    "\n",
    "# # Create a list of dictionaries containing the data\n",
    "# data_list = [{'column_ablated': column, 'iteration': iteration, 'epoch': epoch, 'src': src, 'aucroc': aucroc}\n",
    "#              for column, iteration, epoch, src, aucroc in zip(column_ablated, iterations, epochs, srcs, aucrocs)]\n",
    "\n",
    "# # Create a DataFrame from the list of dictionaries\n",
    "# data = pd.DataFrame(data_list)\n",
    "\n",
    "# # Convert data types if needed\n",
    "# data['iteration'] = data['iteration'].astype(int)\n",
    "# data['epoch'] = data['epoch'].astype(int)\n",
    "# data['aucroc'] = data['aucroc'].astype(float)\n",
    "# data.to_csv('domias_kde_col_abl_UCI_TVAE_fix.csv')\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean_values = data.groupby('column_ablated')['aucroc'].mean()\n",
    "\n",
    "# print(mean_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.barplot(data=data, x='column_ablated', y='aucroc' )\n",
    "\n",
    "# # Customize the plot\n",
    "# plt.xlabel('Column ablated')\n",
    "# plt.ylabel('AUC-ROC')\n",
    "# plt.title('MIA Performance for Column Ablation Study')\n",
    "# plt.ylim(ymin=0.475, ymax=0.525)\n",
    "# #plt.legend(title='src')\n",
    "# #plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col, results in leave_one_out_results.items():\n",
    "#     print(f\"Column Omitted {col}:\")\n",
    "#     for iteration, iteration_results in results.items():\n",
    "#         print(f\"Iteration {iteration}:\")\n",
    "#         for training_size, size_results in iteration_results.items():\n",
    "#             print(f\"Training Size {training_size}:\")\n",
    "#             for training_epoch, mia_performance in size_results.items():\n",
    "#                 print(f\"Training Epoch {training_epoch}: MIA Performance = {mia_performance}\")\n",
    "#             print()\n",
    "\n",
    "\n",
    "# # Create empty lists to store the results\n",
    "# epochs = []\n",
    "# srcs = []\n",
    "# aucrocs = []\n",
    "\n",
    "\n",
    "# # Iterate over the results\n",
    "# for col, results in leave_one_out_results.items():\n",
    "#     for iteration, iteration_results in results.items():\n",
    "#         for training_size, size_results in iteration_results.items():\n",
    "#             for training_epoch, mia_performance in size_results.items():\n",
    "#                 epochs.append(training_epoch)\n",
    "#                 srcs.append(list(mia_performance.keys()))\n",
    "#                 aucrocs.append([value['aucroc'] for value in mia_performance.values()])\n",
    "\n",
    "# # Create a DataFrame from the lists\n",
    "# data = pd.DataFrame({'epoch': epochs, 'src': srcs, 'aucroc': aucrocs})\n",
    "\n",
    "# # Convert lists to individual rows\n",
    "# data = data.explode('src').explode('aucroc')\n",
    "\n",
    "# # Convert data types\n",
    "# data['epoch'] = data['epoch'].astype(int)\n",
    "# data['aucroc'] = data['aucroc'].astype(float)\n",
    "\n",
    "# #print(data.head())\n",
    "\n",
    "# #Filtering for just domias results\n",
    "# filtered_df = data[data['src'] == 'domias']\n",
    "\n",
    "# filtered_df.reset_index(inplace=True)\n",
    "\n",
    "\n",
    "# print(filtered_df.head())\n",
    "# print(filtered_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_df[filtered_df['index'] == 0].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.barplot(data=filtered_df, x='index', y='aucroc')\n",
    "\n",
    "# # Customize the plot\n",
    "# plt.xlabel('Column ablated')\n",
    "# plt.ylabel('AUC-ROC')\n",
    "# plt.title('MIA Performance for Column Ablation Study')\n",
    "# plt.ylim(ymin=0.4, ymax=0.6)\n",
    "# #plt.legend(title='src')\n",
    "# #plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #         def __init__(self) -> None:\n",
    "# #             if gan_method == \"TVAE\":\n",
    "# #                 syn_model = TVAESynthesizer(metadata, epochs=epochs)\n",
    "# #             elif gan_method == \"CTGAN\":\n",
    "# #                 syn_model = CTGAN(epochs=epochs)\n",
    "# #             elif gan_method == \"KDE\":\n",
    "# #                 syn_model = None\n",
    "# #             else:\n",
    "# #                 raise RuntimeError()\n",
    "# #             self.method = gan_method\n",
    "# #             self.model = syn_model\n",
    "\n",
    "\n",
    "\n",
    "# def get_generator(gan_method: str = \"TVAE\", epochs: int = 1000, metadata = None) -> GeneratorInterface:\n",
    "#     class LocalGenerator(GeneratorInterface):\n",
    "#         def __init__(self, metadata=None) -> None:\n",
    "#             self.metadata = metadata\n",
    "#             if gan_method == \"TVAE\":\n",
    "#                 syn_model = TVAESynthesizer(metadata, epochs=epochs)\n",
    "#             elif gan_method == \"KDE\":\n",
    "#                 syn_model = None\n",
    "#             else:\n",
    "#                 raise RuntimeError()\n",
    "                \n",
    "#             self.method = gan_method\n",
    "#             self.model = syn_model\n",
    "\n",
    "#         def fit(self, data: pd.DataFrame) -> \"LocalGenerator\":\n",
    "#             if gan_method == \"KDE\":\n",
    "#                 self.model = stats.gaussian_kde(np.transpose(data))\n",
    "#             else:\n",
    "#                 self.model.fit(data)\n",
    "#             return self\n",
    "\n",
    "#         def generate(self, count: int) -> pd.DataFrame:\n",
    "#             if gan_method == \"KDE\":\n",
    "#                 samples = pd.DataFrame(self.model.resample(count).transpose(1, 0))\n",
    "#             elif gan_method == \"TVAE\":\n",
    "#                 samples = self.model.sample(count)\n",
    "#             else:\n",
    "#                 raise RuntimeError()\n",
    "\n",
    "#             return samples\n",
    "\n",
    "#     return LocalGenerator()\n",
    "\n",
    "\n",
    "# leave_one_out_results = {}\n",
    "# metadata_objects = {}\n",
    "\n",
    "# for i, col in enumerate(df_dataset.columns):\n",
    "#     print(col)\n",
    "    \n",
    "#     df_temp = df_dataset.copy()\n",
    "#     df_temp.drop(columns=[col], inplace=True)\n",
    "#     arr_temp = np.array(df_temp)\n",
    "\n",
    "#     # Create a new metadata object for each loop iteration with a unique name\n",
    "#     metadata_name = f\"metadata_{col}\"\n",
    "#     metadata_objects[metadata_name] = SingleTableMetadata()\n",
    "#     metadata_objects[metadata_name].detect_from_dataframe(data=df_temp)\n",
    "#     print(type(metadata_objects[metadata_name]))\n",
    "\n",
    "#     # Create the generator using the corresponding metadata\n",
    "#     generator = get_generator(\n",
    "#         epochs=training_epochs[0],  # Assuming you only use the first epoch in the list\n",
    "#         metadata=metadata_objects[metadata_name]\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # mem_set_size = 1000 -> originally what training size was\n",
    "# reference_set_size = 10000 #held out set\n",
    "# training_epochs = [2000]\n",
    "# training_sizes = [10000]\n",
    "# #synthetic_sizes = [200]\n",
    "# density_estimator = \"kde\"  # prior, kde, bnaf\n",
    "# gen_size = 500 #same as synthetic_sizes\n",
    "\n",
    "# method = \"TVAE\"\n",
    "\n",
    "# leave_one_out_results = {}\n",
    "# metadata_objects={}\n",
    "\n",
    "\n",
    "# leave_one_out_results = {}\n",
    "# metadata_objects = {}\n",
    "\n",
    "# for i, col in enumerate(df_dataset.columns):\n",
    "#     print(col)\n",
    "    \n",
    "#     df_temp = df_dataset.copy()\n",
    "#     df_temp.drop(columns=[col], inplace=True)\n",
    "#     arr_temp = np.array(df_temp)\n",
    "\n",
    "#     # Create a new metadata object for each loop iteration with a unique name\n",
    "#     metadata_name = f\"metadata_{col}\"\n",
    "#     metadata_objects[metadata_name] = SingleTableMetadata()\n",
    "#     metadata = metadata_objects[metadata_name].detect_from_dataframe(data=df_temp)\n",
    "\n",
    "#     # Create the generator using the corresponding metadata\n",
    "#     generator = get_generator(\n",
    "#         gan_method=method,\n",
    "#         epochs=training_epochs[0],  # Assuming you only use the first epoch in the list\n",
    "#         metadata=metadata,\n",
    "#     )\n",
    "\n",
    "#     # Rest of your code for evaluation and storing results...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "LIMIT_BAL\n",
      "(30000, 23)\n",
      "             0         1         2         3         4         5         6   \\\n",
      "0     -1.234323  0.185828 -1.057295 -0.486615  0.904712  1.782348  0.138865   \n",
      "1      0.810161  0.185828 -1.057295 -0.269643  0.014861  0.111736  0.138865   \n",
      "2      0.810161 -1.079457  0.858557  1.900084  0.014861  0.111736  0.138865   \n",
      "3      0.810161  0.185828 -1.057295  0.055816  0.014861  0.111736  0.138865   \n",
      "4      0.810161  0.185828 -1.057295 -0.052670  0.014861  0.111736  0.138865   \n",
      "...         ...       ...       ...       ...       ...       ...       ...   \n",
      "29995  0.810161  0.185828  0.858557 -1.029047  0.014861  0.111736  0.138865   \n",
      "29996  0.810161 -1.079457 -1.057295  1.683111  0.014861  0.111736  0.138865   \n",
      "29997  0.810161 -1.079457 -1.057295 -0.269643 -1.764843 -1.558876 -1.532192   \n",
      "29998  0.810161  0.185828 -1.057295  1.249166 -1.764843 -1.558876 -1.532192   \n",
      "29999  0.810161  1.451114 -1.057295  0.706734  0.014861  0.111736  0.138865   \n",
      "\n",
      "             7         8         9   ...        13        14        15  \\\n",
      "0      0.188746  0.234917  0.253137  ...  0.116180  0.172965  0.201236   \n",
      "1      0.188746  0.234917  0.253137  ... -0.620532 -0.638221 -0.652724   \n",
      "2      0.188746  0.234917  0.253137  ... -0.243161 -0.130722 -0.081117   \n",
      "3      0.188746  0.234917  0.253137  ... -0.045171  0.055128  0.232788   \n",
      "4      0.188746  0.234917 -0.616452  ... -0.583070 -0.625293 -0.629249   \n",
      "...         ...       ...       ...  ...       ...       ...       ...   \n",
      "29995 -1.521944 -1.530046 -0.616452  ... -0.672497 -0.663059 -0.572124   \n",
      "29996  0.188746  0.234917  0.253137  ...  0.744267  0.614243  0.665729   \n",
      "29997 -1.521944 -1.530046 -1.486041  ... -0.672497 -0.663059 -0.652724   \n",
      "29998 -1.521944 -1.530046 -1.486041  ... -0.672497 -0.663059 -0.652724   \n",
      "29999  0.188746  0.234917  0.253137  ... -0.533406 -0.512671 -0.551923   \n",
      "\n",
      "             16        17        18        19        20        21        22  \n",
      "0     -0.341942 -0.165846 -0.194567 -0.186780 -0.176684 -0.192129 -0.532942  \n",
      "1     -0.221191 -0.191887 -0.239607 -0.307616 -0.314136 -0.293382 -0.532942  \n",
      "2     -0.040064 -0.039980 -0.012818  0.011102  0.013131 -0.228074 -0.532942  \n",
      "3      0.563691 -0.126784 -0.126411 -0.052731  0.340398  1.112916  1.876378  \n",
      "4     -0.082750 -0.213414 -0.290270 -0.305126 -0.222632  0.091212 -0.532942  \n",
      "...         ...       ...       ...       ...       ...       ...       ...  \n",
      "29995 -0.251378 -0.256990 -0.296801 -0.308063  0.000040 -0.293382 -0.532942  \n",
      "29996 -0.148740 -0.109423 -0.086654 -0.129330 -0.124321 -0.119001 -0.532942  \n",
      "29997 -0.341942 -0.256990 -0.296801 -0.308063 -0.314136 -0.293382  1.876378  \n",
      "29998 -0.341942 -0.256990 -0.296801 -0.308063 -0.314136 -0.293382 -0.532942  \n",
      "29999 -0.264782 -0.209855 -0.263802 -0.290636 -0.299278 -0.284269 -0.532942  \n",
      "\n",
      "[30000 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "arr_uci = np.array(df_uci)\n",
    "def get_dataset() -> np.ndarray:\n",
    "    def data_loader() -> np.ndarray:\n",
    "        scaler = StandardScaler()\n",
    "        X =arr_uci\n",
    "        np.random.shuffle(X)\n",
    "        return scaler.fit_transform(X)\n",
    "\n",
    "    return data_loader()\n",
    "\n",
    "dataset = get_dataset()\n",
    "df_dataset = pd.DataFrame(dataset)\n",
    "\n",
    "df_dataset.rename(columns={0: '0', 1: '1', 2: '2', 3: '3', 4: '4', 5: '5', 6: '6', 7: '7', 8:'8', 9:'9', 10:'10', 11:'11', 12:'12',\n",
    "                        13:'13', 14:'14', 15:'15', 16:'16', 17:'17', 18:'18', 19:'19', 20:'20', 21:'21', 22:'22', 23:'23'}, inplace = True)\n",
    "\n",
    "# metadata = SingleTableMetadata()\n",
    "# metadata.detect_from_dataframe(data=df_dataset)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #Create loop to create a metadta object for each column ablated dataframe\n",
    "# metadata_objects={}\n",
    "# for i, col in enumerate(df_dataset.columns):\n",
    "    \n",
    "#     df_temp = df_dataset.copy()\n",
    "#     df_temp.drop(columns=[col], inplace=True)\n",
    "    \n",
    "#     metadata_name = f\"metadata_{col}\"\n",
    "#     metadata_objects[metadata_name] = SingleTableMetadata()\n",
    "#     metadata_objects[metadata_name].detect_from_dataframe(data=df_temp)\n",
    "\n",
    "# mem_set_size = 1000 -> originally what training size was\n",
    "reference_set_size = 3000 #held out set\n",
    "training_epochs = [1000]\n",
    "training_sizes = [3000]\n",
    "#synthetic_sizes = [200]\n",
    "density_estimator = \"kde\"  # prior, kde, bnaf\n",
    "gen_size = 500 #same as synthetic_sizes\n",
    "\n",
    "method = \"TVAE\"\n",
    "\n",
    "leave_one_out_results = {}\n",
    "\n",
    "\n",
    "for i, col in enumerate(df_uci):\n",
    "    print(i)\n",
    "    print(col)\n",
    "    \n",
    "    \n",
    "    df_temp = df_uci.drop(df_uci.columns[i], axis=1)\n",
    "    arr_uci = np.array(df_temp)\n",
    "    print(arr_uci.shape)\n",
    "    def get_dataset() -> np.ndarray:\n",
    "        def data_loader() -> np.ndarray:\n",
    "            scaler = StandardScaler()\n",
    "            X =arr_uci\n",
    "            np.random.shuffle(X)\n",
    "            return scaler.fit_transform(X)\n",
    "\n",
    "        return data_loader()\n",
    "\n",
    "    dataset = get_dataset()\n",
    "    df_dataset = pd.DataFrame(dataset)\n",
    "    print(df_dataset)\n",
    "\n",
    "    df_dataset.rename(columns={0: '0', 1: '1', 2: '2', 3: '3', 4: '4', 5: '5', 6: '6', 7: '7', 8:'8', 9:'9', 10:'10', 11:'11', 12:'12',\n",
    "                            13:'13', 14:'14', 15:'15', 16:'16', 17:'17', 18:'18', 19:'19', 20:'20', 21:'21', 22:'22'}, inplace = True)\n",
    "\n",
    "    metadata = SingleTableMetadata()\n",
    "    metadata.detect_from_dataframe(data=df_dataset)\n",
    "\n",
    "     \n",
    "    arr_temp = np.array(df_dataset)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    # Create a dictionary to store the results for the current column\n",
    "    column_results = {}\n",
    "\n",
    "    # Set the number of iterations\n",
    "    num_iterations = 5\n",
    "\n",
    "    for iteration in range(1, num_iterations+1):\n",
    "        # Initialize the result dictionary for the current iteration\n",
    "        iteration_results = {}\n",
    "\n",
    "        for training_size in training_sizes:\n",
    "            # Initialize the result dictionary for the current training size\n",
    "            size_results = {}\n",
    "\n",
    "            for training_epoch in training_epochs:\n",
    "                generator = get_generator(\n",
    "                    gan_method=method,\n",
    "                    epochs=training_epoch,\n",
    "                )\n",
    "\n",
    "                perf = evaluate_performance(\n",
    "                    generator,\n",
    "                    arr_temp,\n",
    "                    training_size,\n",
    "                    reference_set_size,\n",
    "                    training_epochs=training_epoch,\n",
    "                    synthetic_sizes=[gen_size],\n",
    "                    density_estimator=density_estimator,\n",
    "                )\n",
    "\n",
    "                # Store the MIA performance for the current training size and epoch\n",
    "                size_results[training_epoch] = perf[gen_size][\"MIA_performance\"]\n",
    "\n",
    "            # Store the results for the current training size\n",
    "            iteration_results[training_size] = size_results\n",
    "\n",
    "        # Store the results for the current iteration\n",
    "        column_results[iteration] = iteration_results\n",
    "    \n",
    "    # Store the results for the current column\n",
    "    leave_one_out_results[col] = column_results\n",
    "    \n",
    "\n",
    "# Print the results\n",
    "for col, results in leave_one_out_results.items():\n",
    "    print(f\"Column Omitted {col}:\")\n",
    "    for iteration, iteration_results in results.items():\n",
    "        print(f\"Iteration {iteration}:\")\n",
    "        for training_size, size_results in iteration_results.items():\n",
    "            print(f\"Training Size {training_size}:\")\n",
    "            for training_epoch, mia_performance in size_results.items():\n",
    "                print(f\"Training Epoch {training_epoch}: MIA Performance = {mia_performance}\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty lists to store the results\n",
    "iterations = []\n",
    "epochs = []\n",
    "srcs = []\n",
    "aucrocs = []\n",
    "column_ablated = []\n",
    "\n",
    "# Iterate over the results\n",
    "for col, results in leave_one_out_results.items():\n",
    "    for iteration, iteration_results in results.items():\n",
    "        for training_size, size_results in iteration_results.items():\n",
    "            for training_epoch, mia_performance in size_results.items():\n",
    "                # Append values to the lists\n",
    "                iterations.append(iteration)\n",
    "                epochs.append(training_epoch)\n",
    "                srcs.append(list(mia_performance.keys())[8])\n",
    "                aucrocs.append(list(mia_performance.values())[8]['aucroc'])\n",
    "                column_ablated.append(col)  # Add the column_ablated value\n",
    "\n",
    "# Create a list of dictionaries containing the data\n",
    "data_list = [{'column_ablated': column, 'iteration': iteration, 'epoch': epoch, 'src': src, 'aucroc': aucroc}\n",
    "             for column, iteration, epoch, src, aucroc in zip(column_ablated, iterations, epochs, srcs, aucrocs)]\n",
    "\n",
    "# Create a DataFrame from the list of dictionaries\n",
    "data = pd.DataFrame(data_list)\n",
    "\n",
    "# Convert data types if needed\n",
    "data['iteration'] = data['iteration'].astype(int)\n",
    "data['epoch'] = data['epoch'].astype(int)\n",
    "data['aucroc'] = data['aucroc'].astype(float)\n",
    "data.to_csv('domias_kde_col_abl_UCI_TVAE_fix.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "sns.barplot(data=data, x='column_ablated', y='aucroc' )\n",
    "\n",
    "# Customize the plot\n",
    "plt.xlabel('Column ablated')\n",
    "plt.ylabel('AUC-ROC')\n",
    "plt.title('MIA Performance for Column Ablation Study')\n",
    "plt.ylim(ymin=0.4, ymax=0.65)\n",
    "#plt.legend(title='src')\n",
    "#plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_values = data.groupby('column_ablated')['aucroc'].mean()\n",
    "\n",
    "print(mean_values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
